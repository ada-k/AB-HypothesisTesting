# -*- coding: utf-8 -*-
"""ML_a_b_testing_latest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19B6wYGR1j0ekOIFMJgQHmgC0g6hXS-VF

## A/B Testing with Machine Learning
Machine Learning enables modelling of complex systems unlike the statistical inference approach.

Feature significance is what tells whether the experiment had some impact and also the contribution of other features. 

## Data
The BIO data for this project is a “Yes” and “No” response of online users to the following question:


`Q: Do you know the brand SmartAd?`

      Yes
      No

The data has the following columns:
  * **auction_id**: the unique id of the online user who has been presented the BIO. 
  * **experiment**: which group the user belongs to - control or exposed.
  * **date**: the date in YYYY-MM-DD format
  * **hour**: the hour of the day in HH format.
  * **device_make**: the name of the type of device the user has e.g. Samsung
  * **platform_os**: the id of the OS the user has. 
  * **browser**: the name of the browser the user uses to see the BIO questionnaire.
  * **yes**: 1 if the user chooses the “Yes” radio button for the BIO questionnaire.
  * **no**: 1 if the user chooses the “No” radio button for the BIO questionnaire.

## 1.  Libraries
"""

# data processing and Linear Algebra
import pandas as pd
import numpy as np

# plotting 
import seaborn as sns
import matplotlib.pyplot as plt

# google authentication
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# maths and statistics
from scipy import stats
from scipy.stats import skew, norm
import math

# ML models 
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

# House keeping (data preparation and model evaluation)
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.metrics import confusion_matrix, log_loss


# others
import datetime as dt

# ignore warnings
import warnings
warnings.filterwarnings(action="ignore")

"""## 2. Dataset"""

# function to fetch data
def fetch_data(id, file_name):
  auth.authenticate_user()
  gauth = GoogleAuth()
  gauth.credentials = GoogleCredentials.get_application_default()
  drive = GoogleDrive(gauth)

  downloaded = drive.CreateFile({'id':id}) 
  downloaded.GetContentFile(file_name)

  data=pd.read_csv(file_name)
  return data

# fetch the data
data = fetch_data('1YSn01vvlHKQaAIBtwIXRNd-oTaTuDN09', 'ABAdRecall.csv')
data.head()

"""## 3. Preprocessing

### 3.1 Null values
"""

# Null Values
data.isna().any()

"""No null values, good to go.

### 3.2 Numerical + Categorical Features
"""

categorical = []
numerical = []
for col in data.columns:
  if data[col].dtype == object:
    categorical.append(col)
  elif data[col].dtype in ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']:
    numerical.append(col)


features = categorical + numerical 
df = data[features]
df.head()

"""### 3.3 Outlier detection"""

# check if they exist
def iqr_outlier_test(data, col):
  Q1 = np.percentile(data[col], 25, interpolation = 'midpoint')  
  Q2 = np.percentile(data[col], 50, interpolation = 'midpoint')  
  Q3 = np.percentile(data[col], 75, interpolation = 'midpoint')  
  IQR = stats.iqr(data[col], interpolation = 'midpoint') 
  o = (data[col] < (Q1 - 1.5 * IQR)) |(data[col] > (Q3 + 1.5 * IQR))
  m = o.unique()
  return m

for col in df[numerical].columns:
  print(col, '-', iqr_outlier_test(df, col))

"""3 numerical features have outliers.They cannot be treated since the features are representative of cateorical features.

The code below would treat them using the 90th and 10th percentiles.
"""

# #treat them
# def treat_outliers(data, col):
#   data[col] = data[col].clip(lower=data[col].quantile(0.10), upper= data[col].quantile(0.90))

# for col in df[numerical].columns:
#   treat_outliers(df, col)

# #check again
# for col in df[numerical].columns:
#   print(col, '-', iqr_outlier_test(df, col))

"""### 3.4 Skewness + Normalization

Linear models love normally distributed data, skewness is usually checked to ensure normality of the data.

But: 
We do not expect skewed features since most of them are categorical. The few numerical ones are either representative of categorical data( platform OS, yes, no) or are timestamps (hour).


Nevertheless...
"""

# Find skewed numerical features
skew_features = df[numerical].apply(lambda x: skew(x)).sort_values(ascending=False)

high_skew = skew_features[skew_features > 0.5]
skew_index = high_skew.index

print("There are {} numerical features with Skew > 0.5 :".format(high_skew.shape[0]))
skewness = pd.DataFrame({'Skew' :high_skew})
skew_features

"""The skewness are extremes (super high or super low), except for the hour variable."""

# f, ax = plt.subplots(figsize=(7, 6))
# sns.distplot(df['platform_os'], bins = 20, color = 'blue')
# ax.set(ylabel="Frequency")
# ax.set(xlabel="platform_os")
# ax.set(title="platform_os distribution")

# print(df.platform_os.nunique())

# f, ax = plt.subplots(figsize=(7, 6))
# sns.distplot(df['hour'], bins = 20, color = 'Magenta')
# ax.set(ylabel="Frequency")
# ax.set(xlabel="hour")
# ax.set(title="hour distribution")

# df.hour.value_counts()

"""### 3.5 Correlation

Checking correlation to avoid multicolinearity issues in predictor features.

We do not expect the features to be correlated.
"""

def correlation_map(f_data, f_feature, f_number):
    f_most_correlated = f_data.corr().nlargest(f_number,f_feature)[f_feature].index
    f_correlation = f_data[f_most_correlated].corr()
    
    f_mask = np.zeros_like(f_correlation)
    f_mask[np.triu_indices_from(f_mask)] = True
    with sns.axes_style("white"):
        f_fig, f_ax = plt.subplots(figsize=(8, 6))
        f_ax = sns.heatmap(f_correlation, mask=f_mask, vmin=0, vmax=1, square=True,
                           annot=True, annot_kws={"size": 10}, cmap="BuPu")

    plt.show()

correlation_map(df, 'yes', 4)

"""Like we expected, so no multicolinearity worries.

### 3.6 Feature Generation and Reduction
"""

# Feature generation
df['date'] = pd.to_datetime(df['date'],format='%Y-%m-%d')  # convert date to datetime object
# df['day']=df['date'].dt.day                       #extract the day
df['dayofweek_num']=df['date'].dt.dayofweek       # extract the day of the week

# features reduction
df = df.drop(['date'], axis = 1)  #drop  the date col
df = df.drop(['auction_id'], axis = 1)  #drop  the auction_id col
df.tail(5)

"""### 3.7 Remapping categorical variables"""

#check the datatypes
print(df.shape)
df.dtypes

# get the location of the 3 categorical columns
features = df.copy()
indices = []
for col in ['browser', 'experiment', 'device_make']:
    k = features.columns.get_loc(col)
    indices.append(k)
    
indices

# Encoding categorical variables using Label Encoder
columns = indices
for col in columns:
    x = features.iloc[:, col].values
    x = x.reshape(-1,1)
    encoder = LabelEncoder()
    encoder = encoder.fit(x)
    x = encoder.transform(x)
    features.iloc[:, col] = x 

# features = pd.get_dummies(df)
print(features.shape)
features.head()

"""## 4. Modelling

### 4.1 Train , Validation and Test Sets (70, 20, 10)
Predictor + target variables
"""

# create the target variable from the yes/no cols then drop yes/no cols

# the 1s in yes remain the same, the 1s in no become 2s, the entries with 0s in both cols remain as 0s.
features['target'] = 0
features.loc[features['yes'] ==1, 'target'] = 1
features.loc[features['no'] ==1, 'target'] = 2
features = features.drop(['yes', 'no'], axis = 1)
# features = features[features.target != 0]
# features.loc[features['target'] ==2, 'target'] = 0
print(features.shape)
features.target.value_counts()

features.head()

# dependent and independent variables
x = features.drop(['target'], axis = 1)
y = features[['target']]

# split dataset to train and test sets (90:10)
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = .1, random_state = 0)
print('x train', x_train.shape)
print('y train', y_train.shape)
print('x test', x_test.shape)
print('y test', y_test.shape)

# get the validation set from the train set (70:20)

# the % changes to 22 to be representative of the 20 expected originally
x_train, x_val, y_train, y_val = train_test_split(x_train,y_train, test_size = .22, random_state = 0)
print('x train', x_train.shape)
print('y train', y_train.shape)
print('x validation', x_val.shape)
print('y validation', y_val.shape)
print('x test', x_test.shape)
print('y test', y_test.shape)

"""### 4.2 Logistic Regression

**Logistic regression** is a statistical model that in its basic form uses a logistic function to model a binary dependent variable.
"""

# create the regressor
regressor = LogisticRegression(solver = 'lbfgs', random_state=42)
regressor.fit(x_train, y_train)

scores = cross_val_score(estimator = regressor, X = x_train, y = y_train, cv = 5)
print(scores)
print("mean Logistic regression score : ", scores.mean())

# feature importance
feat_imp_dict = dict(zip(x_train.columns, regressor.coef_[0]))
feat_imp = pd.DataFrame.from_dict(feat_imp_dict, orient='index')
feat_imp.rename(columns = {0:'FeatureImportance'}, inplace = True)
feat_imp.sort_values(by=['FeatureImportance'], ascending=False)

# feature weights for every class
coef_0=regressor.coef_[0]
coef_1=regressor.coef_[1]
coef_2=regressor.coef_[2]
print(coef_0)
print(coef_1)
print(coef_2)

"""### 4.3 XGB
**XGBoost** is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework.

[The documentation.](https://xgboost.readthedocs.io/en/latest/)
"""

### XGB
xgb = XGBClassifier(random_state=42, )
xgb.fit(x_train, y_train)

scores = cross_val_score(estimator = xgb, X = x_train, y = y_train, cv = 5)
print(scores)
print("mean xgb score : ", scores.mean())

# feature importance
feat_imp_dict = dict(zip(x_train.columns, xgb.feature_importances_))
feat_imp_2 = pd.DataFrame.from_dict(feat_imp_dict, orient='index')
feat_imp_2.rename(columns = {0:'FeatureImportance'}, inplace = True)
feat_imp_2.sort_values(by=['FeatureImportance'], ascending=False).head()

"""### 4.4 Decision Trees

A **decision tree** is a decision support tool that uses a tree-like model of decisions and their possible consequences. It is one way to display an algorithm that only contains conditional control statements.

A **decision tree classifier** (Pang-Ning et al., 2006) creates the classification model by building a decision tree. Each node in the tree specifies a test on an attribute, each branch descending from that node corresponds to one of the possible values for that attribute.
"""

### dt
tree = DecisionTreeClassifier(random_state=42)
tree.fit(x_train, y_train)

scores = cross_val_score(estimator = tree, X = x_train, y = y_train, cv = 5)
print(scores)
print("mean decision trees score : ", scores.mean())

# feature importance
feat_importance = tree.tree_.compute_feature_importances(normalize=False)
feat_imp_dict = dict(zip(x_train.columns, tree.feature_importances_))
feat_imp_3 = pd.DataFrame.from_dict(feat_imp_dict, orient='index')
feat_imp_3.rename(columns = {0:'FeatureImportance'}, inplace = True)
feat_imp_3.sort_values(by=['FeatureImportance'], ascending=False).head()

"""**experiment** appears twice in the top 5 most important features out of the 3 algorithms.

### 4.5 Model Performance plot
"""

# create accuracies df then plot
data = {'accuracy': [0.8403598319455925 * 100,  0.8403598319455925 * 100,  0.7856773934443837 * 100], 
        'model': ['Logistic Regression' , 'XGB', 'Decision Trees']}
df = pd.DataFrame(data, columns = ['accuracy', 'model'])
# plot
plt.figure(figsize = (6,4))
sns.barplot(y = df.accuracy, x = df.model)
plt.title('barplot indicating model performances')

"""### 4.6 Loss Functions

It’s a method of evaluating how well an algorithm models the given data. If predictions deviates too much from actual results, loss function will be high.
"""

# log loss for logistic regression
probabilities = regressor.predict_proba(x_val)
# calculate log loss
loss = log_loss(y_val, probabilities)
loss

# log loss for xgb
probabilities = xgb.predict_proba(x_val)
# calculate log loss
loss = log_loss(y_val, probabilities)
loss

# log loss for dt
probabilities = tree.predict_proba(x_val)
# calculate log loss
loss = log_loss(y_val, probabilities)
loss

"""### 4.7 Loss Plots"""

# create accuracies df then plot
data = {'loss': [0.519512717164833,  0.5132259370622342], 
        'model': ['Logistic Regression' , 'XGB']}
df = pd.DataFrame(data, columns = ['loss', 'model'])
# plot
plt.figure(figsize = (6,4))
sns.barplot(y = df.loss, x = df.model)
plt.title('barplot indicating loss functions for different models')

"""### 4.8 Feature Importance Plots"""

# LR
plt.figure(figsize = (6,4))
sns.barplot(y = feat_imp.FeatureImportance, x = feat_imp.index)
plt.title('Feature Importances in Logistic Regression')
plt.xticks(rotation = 45)

# XGB
plt.figure(figsize = (6,4))
sns.barplot(y = feat_imp_2.FeatureImportance, x = feat_imp_2.index)
plt.title('Feature Importances in XGB')
plt.xticks(rotation = 45)

# DT
plt.figure(figsize = (6,4))
sns.barplot(y = feat_imp_3.FeatureImportance, x = feat_imp_3.index)
plt.title('Feature Importances in Decision Trees')
plt.xticks(rotation = 45)

"""### 4.8 Predictions"""

# using Decision Tree to run predictions on x_test
y_pred = tree.predict(x_test)
a = pd.DataFrame(y_pred)
a.columns = ['pred']
a.pred.value_counts()

!pip freeze > requirements.txt